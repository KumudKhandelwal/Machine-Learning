{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSTI_DL_Lab4_October2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXfVRw5p4YOo"
      },
      "source": [
        "# Neural Network on GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT49HHnBMPTh"
      },
      "source": [
        "\n",
        "From Kaggle: \n",
        "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
        "\n",
        "[Read more.](https://www.kaggle.com/c/digit-recognizer)\n",
        "\n",
        "\n",
        "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-Iyt4NXJoU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets \n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq6RJyDIOB-T"
      },
      "source": [
        "## STEP 1: LOADING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL21SXchOBwe"
      },
      "source": [
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_zuJQxSOKQt"
      },
      "source": [
        "## STEP 2: MAKING DATASET ITERABLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bGDbP1pL4XE"
      },
      "source": [
        "batch_size = 100\n",
        "n_iters = 1200 \n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FEft3CQOQIR"
      },
      "source": [
        "## STEP 3: CREATE MODEL CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbEJ-1aAL9d1"
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        # Max pool 1\n",
        "        out = self.maxpool1(out)\n",
        "\n",
        "        # Convolution 2 \n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        # Max pool 2 \n",
        "        out = self.maxpool2(out)\n",
        "\n",
        "        # Resize\n",
        "        # Original size: (100, 32, 7, 7)\n",
        "        # out.size(0): 100\n",
        "        # New out size: (100, 32*7*7)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3owYgg2OWYw"
      },
      "source": [
        "## STEP 4: INSTANTIATE MODEL CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dqM6gd4MA2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6fe818-9176-4076-bf39-a54b7a40bb68"
      },
      "source": [
        "model = CNNModel()\n",
        "\n",
        "####################################################################\n",
        "#  USE GPU FOR MODEL                                               #\n",
        "#  The model must be put on the GPU before declaring the optimizer #\n",
        "####################################################################\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNModel(\n",
              "  (cnn1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ArDN205ObQG"
      },
      "source": [
        "## STEP 5: INSTANTIATE LOSS CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtqarS-WMC5S"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqikNfyhOe7o"
      },
      "source": [
        "## STEP 6: INSTANTIATE OPTIMIZER CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1roaQQwYMFFj"
      },
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0A9mJ3d6-f8"
      },
      "source": [
        "Function to compute the accuracy on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rscNQPz_AfF"
      },
      "source": [
        "### Question: modify the following code to exploit both the GPU and the CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv17qSuGO5x6"
      },
      "source": [
        "def test_model(test_loader, model, device):\n",
        "  # Calculate Accuracy         \n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Iterate through test dataset\n",
        "  for images, labels in test_loader:\n",
        "  #######################\n",
        "  #  USE GPU FOR MODEL  #\n",
        "  #######################\n",
        "    images = images.requires_grad_().to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass only to get logits/output\n",
        "    outputs = model(images)\n",
        "\n",
        "    # Get predictions from the maximum value\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Total number of labels\n",
        "    total += labels.size(0)\n",
        "\n",
        "    #######################\n",
        "    #  USE GPU FOR MODEL  #\n",
        "    #######################\n",
        "    # Total correct predictions\n",
        "    if torch.cuda.is_available():\n",
        "      correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "    else:\n",
        "      correct += (predicted == labels).sum()\n",
        "\n",
        "  accuracy = 100 * float(correct) / float(total)\n",
        "    \n",
        "  return accuracy"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJWjmbnOhff"
      },
      "source": [
        "## STEP 7: TRAIN THE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZh9Bryn-15o"
      },
      "source": [
        "### Question: modify the following code to exploit the GPU instead of the CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ADmEX6UMINe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1081606-6f30-4781-d19e-4790ae902801"
      },
      "source": [
        "%%time\n",
        "# Time execution of a Python statement or expression.\n",
        "# wall time is the actual time taken from the start of a computer program to the end\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        #######################\n",
        "        #  USE GPU FOR MODEL  #\n",
        "        #######################\n",
        "        images = images.requires_grad_().to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy on the test set        \n",
        "            accuracy = test_model(test_loader, model, device)\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy on test set: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.3697643280029297. Accuracy on test set: 89.15\n",
            "Iteration: 1000. Loss: 0.3273230791091919. Accuracy on test set: 91.86\n",
            "CPU times: user 13.3 s, sys: 666 ms, total: 14 s\n",
            "Wall time: 14.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozPdNeJH-Tfq"
      },
      "source": [
        "### Question: compare the wall time on GPU to the wall time on CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwshsSjE-rk_"
      },
      "source": [
        "device = \"cpu\" # and run again the notebook"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gekTtS1p_v2k"
      },
      "source": [
        "### Question: increase the number of epoch until 5 to see if we can expect a better average accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxUym9ODAAzA"
      },
      "source": [
        "n_iters = 3000 # increase n_iters"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}